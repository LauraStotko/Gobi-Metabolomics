---
title: "Gobi"
author: "Marie Hackenberg"
date: "2025-02-03"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Load libraries
```{r}
library(readr)
library(tidyr)
library(dplyr)
library(ggplot2)
```

# Load data
Downloading the bulk data from humet.org as either a csv or an excel file was simple, but the names were swapped, making the files unable to be processed by rstudio. Simply changing the extension solved this issue.
```{r}
data_path <-"../data/raw/postprandial_non_imputed.csv"
info_path <- "../data/raw/postprandial_info.csv"
data <- read.csv(data_path, sep = ";", header = TRUE)
info <- read.csv(info_path, sep = ";", header = TRUE)

# Remove numbering columns
data <- subset(data, select = -X)
info <- subset(info, select = -X)

# Check for missing data
sum(is.na(data)) # 4465 missing values, check where they are
data_with_na <- data[apply(is.na(data), 1, any), ] # some timepoints missing, to be imputed later
sum(is.na(info)) # fifteen missing values, check where they are
info_with_na <- info[apply(is.na(info), 1, any), ] # only missing sub-pathways, no action necessary

# Convert value types
options(digits = 14) # set decimal precision to original precision in raw data
convert_to_numeric <- function(x) {
  as.numeric(gsub(",", ".", x)) # R uses . for decimal numbers
}
data <- data %>%
  mutate(across(
    -c(subject, challenge_time, challenge), # exclude subject, challenge_time, and challenge
    convert_to_numeric # apply conversion function
  ))

```
# Pre-processing

Downloaded data was already imputed using missForest.
Next, filter for time points measured after the first 30 minutes of a study challenge. Then, filter for outliers defined as data points beyond four standard deviations from the mean. The order is crucial, as filtering first for outliers and then for time points could exclude to many time points that are outliers simply *because* they stem from the first thirty minutes.


## First 30mins Then 4 SD from mean
```{r}
# Filter for time points within first 30 mins
data_after_30min <- data %>% filter(challenge_time > 30)

# Filter for outliers based on standard deviation
identify_outliers <- function(metabolite_data){
  mean_value <- mean(metabolite_data, na.rm = TRUE)
  sd_value <- sd(metabolite_data, na.rm = TRUE)
  
  # Outliers defined as data points beyond 4 SD from mean
  outliers <- metabolite_data > (mean_value + 4 * sd_value) | metabolite_data < (mean_value - 4 * sd_value)
  
  return(outliers)
}

# Apply outliers function to each metabolite column, excluding subject and challenge_time
outliers_data_after_30min <- data_after_30min %>%
  select(-subject, -challenge_time, -challenge) %>% # remove subject and challenge_time
  apply(2, identify_outliers) # apply outlier detection for each metabolite, "2" for column

# Check how many outliers (TRUE)
table(outliers_data_after_30min)

```
122 data points fit both time and outlier criteria and require manual inspection.


## First 4 SD from mean Then 30mins
```{r}
# Apply outliers function to each metabolite column, excluding subject and challenge_time
outliers_data <- data %>%
  select(-subject, -challenge_time, -challenge) %>% # remove subject and challenge_time
  apply(2, identify_outliers) # apply outlier detection for each metabolite, "2" for column
outliers_data <- as.data.frame(outliers_data)
outliers_data$challenge_time <- data$challenge_time

# Filter for time points within first 30 mins
after_30min_outliers_data <- outliers_data %>% filter(challenge_time > 30)

# Check how many outliers (TRUE)
dim(after_30min_outliers_data)
#table(after_30min_outliers_data)
true_count_total <- sum(after_30min_outliers_data == TRUE, na.rm = TRUE)

# Count FALSE values for each column
false_count_total <- sum(after_30min_outliers_data == FALSE, na.rm = TRUE)

# Print results
cat("Total FALSE values:", false_count_total, "\n")
cat("Total TRUE values:", true_count_total, "\n")

```
Using outliers_data_after_30min for now, with 122 True


## Manual inspection
```{r}
# Combine outlier data with subject and challenge time for manual inspection
outliers_data_df <- as.data.frame(outliers_data_after_30min) # convert to dataframe

metabolite_columns <- data_after_30min %>%
  select(-subject, -challenge_time, -challenge) %>%
  colnames()
colnames(outliers_data_df) <- metabolite_columns

table(unlist(outliers_data_df))
```
## Check for unique subjects per time point, challenge, and metabolite
```{r}
outliers_data_df <- outliers_data_df %>%
  mutate(across(everything(), ~ as.logical(.))) # ensure rue/false values

# Add non-metabolite columns back
outliers_data_df$subject <- data_after_30min$subject
outliers_data_df$challenge_time <- data_after_30min$challenge_time
outliers_data_df$challenge <- data_after_30min$challenge

# Pivot dataframe into long format
outliers_long <- outliers_data_df %>%
  mutate(subject = data_after_30min$subject,
         challenge_time = data_after_30min$challenge_time,
         challenge = data_after_30min$challenge) %>%
  pivot_longer(
    cols = -c(subject, challenge_time, challenge), # keep subject, time, and challenge; make all metabolites long
    names_to = "metabolite",
    values_to = "is_outlier"
  ) %>%
  filter(is_outlier == TRUE) # keep only outlier rows

# Count unique subjects for each metabolite, time, and challenge
outlier_counts <- outliers_long %>%
  group_by(metabolite, challenge_time, challenge) %>%
  summarise(unique_subjects = n_distinct(subject), .groups = "drop") %>% # count unique subjects; "drop" for clean dataframe
  arrange(metabolite, challenge_time, challenge) # order results

# Identify individual subjects that had an outlier at each time for given metabolite
single_subject_outliers <- outlier_counts %>%
  filter(unique_subjects == 1) # keep only cases where **one** subject has outlier
```
Exclude (i.e. set to missing) the identified unique subjects per time point, challenge, and metabolite from further analysis.
```{r}

```

```{r}

```

```{r}

```

